# Open Gemini Deep Research

A powerful open-source research assistant powered by Google's Gemini AI that performs deep, multi-layered research on any topic.

## Features

- Automated deep research with adjustable breadth and depth
- Follow-up question generation for better context
- Concurrent processing of multiple research queries
- Comprehensive final report generation with citations
- Three research modes: fast, balanced, and comprehensive
- Progress tracking and detailed logging
- Source tracking and citation management

## Prerequisites

- Python 3.9+
- Google Gemini API key
- Docker (if using dev container)
- VS Code with Dev Containers extension (if using dev container)

## Installation

You can set up this project in one of two ways:

### Option 1: Using Dev Container (Recommended)

1. Open the project in VS Code
2. When prompted, click "Reopen in Container" or run the "Dev Containers: Reopen in Container" command
3. Create a `.env` file in the root directory and add your Gemini API key:
   ```
   GEMINI_KEY=your_api_key_here
   ```

### Option 2: Local Installation

1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd open-gemini-deep-research
   ```

2. Create and activate a virtual environment (recommended):
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Create a `.env` file in the root directory and add your Gemini API key:
   ```
   GEMINI_KEY=your_api_key_here
   ```

## Usage

Run the main script with your research query:
```bash
python main.py "your research query here"
```

### Optional Arguments

- `--mode`: Research mode (choices: fast, balanced, comprehensive) [default: balanced]
- `--num-queries`: Number of queries to generate [default: 3]
- `--learnings`: List of previous learnings [optional]

Example:
```bash
python main.py "Impact of artificial intelligence on healthcare" --mode comprehensive --num-queries 5
```


## Output

The script will:
1. Analyze your query for optimal research parameters
2. Ask follow-up questions for clarification
3. Conduct multi-layered research
4. Generate a comprehensive report saved as `final_report.md`
5. Show progress updates throughout the process

## Project Structure

```
open-gemini-deep-research/
├── .devcontainer/
│   └── devcontainer.json
├── src/
│   ├── __init__.py
│   └── deep_research.py
├── .env
├── .gitignore
├── dockerfile
├── main.py
├── README.md
└── requirements.txt
```

## How It Works

### Research Modes

The application offers three research modes that affect how deeply and broadly the research is conducted:

1. **Fast Mode**
   - Performs quick, surface-level research
   - Maximum of 3 concurrent queries
   - No recursive deep diving
   - Typically generates 2-3 follow-up questions per query
   - Best for time-sensitive queries or initial exploration
   - Processing time: ~1-3 minutes

2. **Balanced Mode** (Default)
   - Provides moderate depth and breadth
   - Maximum of 7 concurrent queries
   - No recursive deep diving
   - Generates 3-5 follow-up questions per query
   - Explores main concepts and their immediate relationships
   - Processing time: ~3-6 minutes
   - Recommended for most research needs

3. **Comprehensive Mode**
   - Conducts exhaustive, in-depth research
   - Maximum of 5 initial queries, but includes recursive deep diving
   - Each query can spawn sub-queries that go deeper into the topic
   - Generates 5-7 follow-up questions with recursive exploration
   - Explores primary, secondary, and tertiary relationships
   - Includes counter-arguments and alternative viewpoints
   - Processing time: ~5-12 minutes
   - Best for academic or detailed analysis

### Research Process

1. **Query Analysis**
   - Analyzes initial query to determine optimal research parameters
   - Assigns breadth (1-10 scale) and depth (1-5 scale) values
   - Adjusts parameters based on query complexity and chosen mode

2. **Query Generation**
   - Creates unique, non-overlapping search queries
   - Uses semantic similarity checking to avoid redundant queries
   - Maintains query history to prevent duplicates
   - Adapts number of queries based on mode settings

3. **Concurrent Processing**
   - Implements async processing with rate limiting
   - Processes multiple queries simultaneously
   - Uses semaphore to control concurrent API calls
   - Maintains progress tracking for each query

4. **Deep Research** (Comprehensive Mode)
   - Implements recursive research strategy
   - Each query can generate follow-up queries
   - Reduces breadth at deeper levels (breadth/2)
   - Maintains visited URLs to avoid duplicates
   - Combines learnings from all levels

5. **Report Generation**
   - Synthesizes findings into a coherent narrative
   - Minimum 3000-word detailed report
   - Includes inline citations and source tracking
   - Organizes information by relevance and relationship
   - Adds creative elements like scenarios and analogies
   - Maintains factual accuracy while being engaging

### Technical Implementation

- Uses Google's Gemini AI for:
  - Query analysis and generation
  - Content processing and synthesis
  - Semantic similarity checking
  - Report generation
- Implements async processing with custom rate limiting
- Uses progress tracking system for detailed logging
- Maintains research tree structure for relationship mapping

#### Rate Limiting with Semaphores

The application uses semaphores for rate limiting concurrent API calls. A semaphore is a synchronization primitive that:
- Controls access to a limited resource (in this case, API calls)
- Maintains a counter of available "slots"
- Blocks new requests when all slots are in use
- Releases slots when operations complete

Example from our implementation:
```python
class AsyncLimit:
    def __init__(self, limit: int):
        # Creates a semaphore with N slots (e.g., 8 concurrent calls)
        self.sem = asyncio.Semaphore(limit)
        self.loop = asyncio.get_event_loop()

    async def run(self, fn: Callable[..., T], *args, **kwargs) -> T:
        # Waits for an available slot before executing
        async with self.sem:
            if asyncio.iscoroutinefunction(fn):
                return await fn(*args, **kwargs)
            else:
                return await self.loop.run_in_executor(None, fn, *args, **kwargs)
```

This ensures we don't overwhelm the Gemini API by:
- Limiting concurrent API calls to 8 at a time
- Queuing additional requests until slots become available
- Automatically managing the release of slots
- Maintaining efficient async processing

#### Research Tree Implementation

The research tree is implemented through a combination of recursive processing and state tracking:

1. **Query Processing Flow**
```python
async def deep_research(self, query: str, breadth: int, depth: int, learnings: list[str] = []):
    # Initialize progress tracker
    progress = ResearchProgress(depth, breadth)
    
    # Limit queries based on mode
    max_queries = {
        "fast": 3,
        "balanced": 7,
        "comprehensive": 5  # Lower due to recursive multiplication
    }[self.mode]
    
    # Generate and process initial queries
    queries = self.generate_queries(
        query,
        min(breadth, max_queries),
        learnings,
        previous_queries=self.query_history
    )
```

2. **State Tracking**
```python
class ResearchProgress:
    def __init__(self, depth: int, breadth: int):
        self.total_depth = depth
        self.total_breadth = breadth
        self.current_depth = depth
        self.current_breadth = 0
        # Track queries at each depth level
        self.queries_by_depth = {}  # {depth: {query: {completed: bool, learnings: list}}}
        self.total_queries = 0
        self.completed_queries = 0
```

3. **Recursive Deep Diving** (Comprehensive Mode Only)
```python
# Only go deeper if in comprehensive mode and depth > 1
if self.mode == "comprehensive" and depth > 1:
    # Reduce breadth for deeper levels
    new_breadth = min(2, math.ceil(breadth / 2))
    new_depth = depth - 1

    if processed_result['follow_up_questions']:
        # Take only the most relevant question
        next_query = processed_result['follow_up_questions'][0]
        
        # Recursive call with accumulated learnings
        sub_results = await self.deep_research(
            query=next_query,
            breadth=new_breadth,
            depth=new_depth,
            learnings=learnings + processed_result["learnings"]
        )
```

Key Features:
- **Query History**: Maintains a set of previously asked queries to avoid duplicates
- **Progress Tracking**: Records state and progress at each depth level
- **Concurrent Processing**: Uses async/await for parallel query processing
- **Depth Control**: Reduces breadth and depth as it goes deeper
- **Learning Accumulation**: Combines learnings from all levels

Example Flow:
1. Initial query generates up to N sub-queries (N depends on mode)
2. Each sub-query is processed concurrently
3. In comprehensive mode, each query can generate one follow-up
4. Follow-ups are processed with reduced breadth/depth
5. All learnings are accumulated back up the tree
6. Progress is tracked and reported at each step

The tree isn't a traditional tree data structure, but rather emerges from the recursive processing pattern and state tracking in the `ResearchProgress` class.
